![img1](https://github.com/zhenglinpan/Awesome-Animation-Research/blob/master/assets/teaser.gif)

[![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/amirhossein-kz/Awesome-Diffusion-Models-in-Medical-Imaging) 
[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](https://opensource.org/licenses/MIT)
[![Teaser](https://img.shields.io/badge/Teaser_by-ÂØùÂõΩ-pink)](https://space.bilibili.com/177312952?spm_id_from=333.337.0.0)
<p align="left">
   ENGLISH | <a href="https://github.com/zhenglinpan/Awesome-Animation-Research/blob/master/README_CN.md">ÁÆÄ‰Ωì‰∏≠Êñá</a>
</p>

# Awesome Animation Research üé•üìö

This repository provides a curated collection of dataset, research, and resources related to **üéûÔ∏ècel animation / üéûÔ∏èhand-drawn cartoons** specifically. 

üíÅ‚Äç‚ôÄÔ∏è**What You'll Find Here:** Papers/Dataset/Repo closely related to cel animation(cartoon video) that could potentially assist creating animation. e.g. Inbetweening, Genga Colorization. 

ü§∑‚Äç‚ôÄÔ∏è**What's Not Included:** General Anime Research. i.e. Anime Style Transfer, Anime Image Enhancement, Anime Image generation. If you are interested in general anime research, please refer to [AwesomeAnimeResearch](https://github.com/SerialLain3170/AwesomeAnimeResearch).

****

üôá‚Äç‚ôÄÔ∏èCreating animation is time-consuming and often involves manual work. AI tools are changing this landscape. Researchers are tackling animation-specific challenges like inbetweening and frame-to-frame color propagation. 

You might notice the repo's list is currently short. Animation research is a relatively new and niche area, and we look forward to more researchers, including you, contributing to its growth.

The repo will keep track of the latest research. Feel free to follow and star ! üåü

## New Papers
<!-- [<span style="color:red">*new</span>]  -->

**Toon3D: Seeing Cartoons from a New Perspective** \
üö©„Äê3D Assistance„Äë*Ethan Weber, Riley Peterlinz, Rohan Mathur, Frederik Warburg, Alexei A. Efros, Angjoo Kanazawa* \
[2024] [Arxiv, 2024]

<a href="https://www.arxiv.org/abs/2405.10320"><img src="./assets/paper.svg" alt="Icon" width="15" height="15"></a> &nbsp;
<a href="https://toon3d.studio/"><img src="./assets/webpage.svg" alt="Icon" width="15" height="15"></a> &nbsp;
<a href="https://www.youtube.com/watch?v=tJ7UKALsF-0&feature=youtu.be"><img src="./assets/demo.svg" alt="Icon" width="15" height="15"></a> &nbsp;
<a href="https://github.com/ethanweber/toon3d"><img src="./assets/repo.svg" alt="Icon" width="15" height="15"></a> &nbsp;
<a href="https://huggingface.co/datasets/ethanweber/toon3d-dataset">ü§ó</a> &nbsp;


üö©„ÄêDataset„Äë**Sakuga-42M Dataset: Scaling Up Cartoon Research** \
*Zhenglin Pan, Yu Zhu, Yuxuan Mu* \
[13 May., 2024] [arXiv, 2024]

<a href="https://arxiv.org/abs/2405.07425"><img src="./assets/paper.svg" alt="Icon" width="15" height="15"></a> &nbsp;
<a href="https://zhenglinpan.github.io/Sakuga_42M/"><img src="./assets/webpage.svg" alt="Icon" width="15" height="15"></a> &nbsp;
<a href="https://github.com/zhenglinpan/SakugaDataset"><img src="./assets/repo.svg" alt="Icon" width="15" height="15"></a> &nbsp;
<a href="https://huggingface.co/datasets/aidenpan/Sakuga-42M"><img src="./assets/dataset.svg" alt="Icon" width="15" height="15"></a> &nbsp;

üö©„ÄêInbetween„Äë **Joint Stroke Tracing and Correspondence for 2D Animation** \
*Haoran Mo, Chengying Gao, Ruomei Wang*\
[9 Apr., 2024] [SIGGRAPH, 2024]

<a href="https://dl.acm.org/doi/10.1145/3649890"><img src="./assets/paper.svg" alt="Icon" width="15" height="15"></a> &nbsp;
<a href="https://markmohr.github.io/JoSTC/"><img src="./assets/webpage.svg" alt="Icon" width="15" height="15"></a> &nbsp;
<a href="https://github.com/MarkMoHR/JoSTC"><img src="./assets/repo.svg" alt="Icon" width="15" height="15"></a> &nbsp;


üö©„ÄêColorization„Äë **Learning Inclusion Matching for Animation Paint Bucket Colorization** \
*Yuekun Dai, Shangchen Zhou, Qinyue Li, Chongyi Li, Chen Change Loy*\
[2024] [CVPR, 2024]

<a href="https://arxiv.org/abs/2403.18342"><img src="./assets/paper.svg" alt="Icon" width="15" height="15"></a> &nbsp;
<a href="https://ykdai.github.io/projects/InclusionMatching"><img src="./assets/webpage.svg" alt="Icon" width="15" height="15"></a> &nbsp;
<a href="https://www.youtube.com/watch?v=nNnPUItGvSo&ab_channel=YuekunDai"><img src="./assets/demo.svg" alt="Icon" width="15" height="15"></a> &nbsp;
<a href="https://github.com/ykdai/BasicPBC"><img src="./assets/repo.svg" alt="Icon" width="15" height="15"></a> &nbsp;
<a href="https://github.com/ykdai/BasicPBC/tree/main/dataset"><img src="./assets/dataset.svg" alt="Icon" width="15" height="15"></a> &nbsp;


## Datasets

**Sakuga-42M Dataset: Scaling Up Cartoon Research** \
*Zhenglin Pan, Yu Zhu, Yuxuan Mu* \
[13 May., 2024] [arXiv, 2024]

<a href="https://arxiv.org/abs/2405.07425"><img src="./assets/paper.svg" alt="Icon" width="15" height="15"></a> &nbsp;
<a href="https://zhenglinpan.github.io/Sakuga_42M/"><img src="./assets/webpage.svg" alt="Icon" width="15" height="15"></a> &nbsp;
<a href="https://github.com/zhenglinpan/SakugaDataset"><img src="./assets/repo.svg" alt="Icon" width="15" height="15"></a> &nbsp;
<a href="https://huggingface.co/datasets/aidenpan/Sakuga-42M"><img src="./assets/dataset.svg" alt="Icon" width="15" height="15"></a> &nbsp;


**AnimeRun: 2D Animation Visual Correspondence from Open Source 3D Movies** \
*Li Siyao, Yuhang Li, Bo Li, Chao Dong, Ziwei Liu, Chen Change Loy* \
[10 Nov., 2022] [NeurIPS, 2022]

<a href="https://arxiv.org/abs/2211.05709"><img src="./assets/paper.svg" alt="Icon" width="15" height="15"></a> &nbsp;
<a href="https://lisiyao21.github.io/projects/AnimeRun"><img src="./assets/webpage.svg" alt="Icon" width="15" height="15"></a> &nbsp;
<a href=""><img src="./assets/demo.svg" alt="Icon" width="15" height="15"></a> &nbsp;
<a href="https://github.com/lisiyao21/AnimeRun"><img src="./assets/repo.svg" alt="Icon" width="15" height="15"></a> &nbsp;
<a href="https://lisiyao21.github.io/projects/AnimeRun"><img src="./assets/dataset.svg" alt="Icon" width="15" height="15"></a> &nbsp;

<a href=""><img src="./assets/paper.svg" alt="Icon" width="15" height="15"></a> &nbsp;
<a href=""><img src="./assets/webpage.svg" alt="Icon" width="15" height="15"></a> &nbsp;
<a href=""><img src="./assets/demo.svg" alt="Icon" width="15" height="15"></a> &nbsp;
<a href=""><img src="./assets/repo.svg" alt="Icon" width="15" height="15"></a> &nbsp;
<a href=""><img src="./assets/dataset.svg" alt="Icon" width="15" height="15"></a> &nbsp;


## Colorization

**Learning Inclusion Matching for Animation Paint Bucket Colorization** \
*Yuekun Dai, Shangchen Zhou, Qinyue Li, Chongyi Li, Chen Change Loy*\
[2024] [CVPR, 2024]

<a href="https://arxiv.org/abs/2403.18342"><img src="./assets/paper.svg" alt="Icon" width="15" height="15"></a> &nbsp;
<a href="https://ykdai.github.io/projects/InclusionMatching"><img src="./assets/webpage.svg" alt="Icon" width="15" height="15"></a> &nbsp;
<a href="https://www.youtube.com/watch?v=nNnPUItGvSo&ab_channel=YuekunDai"><img src="./assets/demo.svg" alt="Icon" width="15" height="15"></a> &nbsp;
<a href="https://github.com/ykdai/BasicPBC"><img src="./assets/repo.svg" alt="Icon" width="15" height="15"></a> &nbsp;
<a href="https://github.com/ykdai/BasicPBC/tree/main/dataset"><img src="./assets/dataset.svg" alt="Icon" width="15" height="15"></a> &nbsp;

**Coloring anime line art videos with transformation region enhancement network** \
*Ning Wang, Muyao Niu, Zhi Dou, Zhihui Wang, Zhiyong Wang, Zhaoyan Ming, Bin Liu, Haojie Li*\
[Sep., 2023] [Elsevier, 2023] 

<a href="https://www.sciencedirect.com/science/article/abs/pii/S0031320323002625"><img src="./assets/paper.svg" alt="Icon" width="15" height="15"></a> &nbsp;


**SketchBetween: Video-to-Video Synthesis for Sprite Animation via Sketches** \
*Dagmar Lukka Loftsd√≥ttir, Matthew Guzdial*\
[1 Sep., 2022] [ECCV, 2022] 

<a href="https://arxiv.org/abs/2209.00185"><img src="./assets/paper.svg" alt="Icon" width="15" height="15"></a> &nbsp;
<a href="https://github.com/ribombee/SketchBetween"><img src="./assets/repo.svg" alt="Icon" width="15" height="15"></a> &nbsp;


**Animation Line Art Colorization Based on Optical Flow Method** \
*Yifeng Yu, Jiangbo Qian, Chong Wang, Yihong Dong, Baisong Liu*\
[27 Aug., 2022] [SSNR, 2022] 

<a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4202289"><img src="./assets/paper.svg" alt="Icon" width="15" height="15"></a> &nbsp;

**The Animation Transformer: Visual Correspondence via Segment Matching** \
*Evan Casey, V√≠ctor P√©rez, Zhuoru Li, Harry Teitelman, Nick Boyajian, Tim Pulver, Mike Manh, William Grisaitis*\
[6 Sep., 2021] [arXiv, 2021]

<a href="https://arxiv.org/abs/2109.02614"><img src="./assets/paper.svg" alt="Icon" width="15" height="15"></a> &nbsp;
<a href="https://cadmium.app/"><img src="./assets/demo.svg" alt="Icon" width="15" height="15"></a> &nbsp;


**Artist-Guided Semiautomatic Animation Colorization** \
*Harrish Thasarathan, Mehran Ebrahimi* \
[22 Jun., 2020] [arXiv, 2020]

<a href="https://arxiv.org/abs/2006.13717"><img src="./assets/paper.svg" alt="Icon" width="15" height="15"></a> &nbsp;


**Line Art Correlation Matching Feature Transfer Network for Automatic Animation Colorization** \
*Zhang Qian, Wang Bo, Wen Wei, Li Hai, Liu Jun Hui* \
[14 Apr., 2020] [arXiv, 2020]

<a href="https://arxiv.org/abs/2004.06718"><img src="./assets/paper.svg" alt="Icon" width="15" height="15"></a> &nbsp;


**Deep Line Art Video Colorization with a Few References** \
*Min Shi, Jia-Qi Zhang, Shu-Yu Chen, Lin Gao, Yu-Kun Lai, Fang-Lue Zhang* \
[24 Mar., 2020] [arXiv, 2020]

<a href="https://arxiv.org/abs/2003.10685"><img src="./assets/paper.svg" alt="Icon" width="15" height="15"></a> &nbsp;


**Automatic Temporally Coherent Video Colorization** \
*Harrish Thasarathan, Kamyar Nazeri, Mehran Ebrahimi*

<a href="https://arxiv.org/abs/1904.09527"><img src="./assets/paper.svg" alt="Icon" width="15" height="15"></a> &nbsp;
<a href="https://github.com/Harry-Thasarathan/TCVC"><img src="./assets/repo.svg" alt="Icon" width="15" height="15"></a> &nbsp;


## Inbetweening & Interpolation

**Joint Stroke Tracing and Correspondence for 2D Animation** \
*Haoran Mo, Chengying Gao, Ruomei Wang*\
[9 Apr., 2024] [SIGGRAPH, 2024]

<a href="https://dl.acm.org/doi/10.1145/3649890"><img src="./assets/paper.svg" alt="Icon" width="15" height="15"></a> &nbsp;
<a href="https://markmohr.github.io/JoSTC/"><img src="./assets/webpage.svg" alt="Icon" width="15" height="15"></a> &nbsp;
<a href="https://github.com/MarkMoHR/JoSTC"><img src="./assets/repo.svg" alt="Icon" width="15" height="15"></a> &nbsp;

**Deep Geometrized Cartoon Line Inbetweening** \
*Li Siyao, Tianpei Gu, Weiye Xiao, Henghui Ding, Ziwei Liu, Chen Change Loy*\
[Nov., 2023] [ICCV 2023]

<a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Siyao_Deep_Geometrized_Cartoon_Line_Inbetweening_ICCV_2023_paper.pdf"><img src="./assets/paper.svg" alt="Icon" width="15" height="15"></a> &nbsp;
<a href="https://www.youtube.com/watch?si=9FViAZUyFdSfZzS5&v=iUF-LsqFKpI&feature=youtu.be"><img src="./assets/demo.svg" alt="Icon" width="15" height="15"></a> &nbsp;
<a href="https://github.com/lisiyao21/AnimeInbet"><img src="./assets/repo.svg" alt="Icon" width="15" height="15"></a> &nbsp;
<a href="https://drive.google.com/file/d/1SNRGajIECxNwRp6ZJ0IlY7AEl2mRm2DR/view"><img src="./assets/dataset.svg" alt="Icon" width="15" height="15"></a> &nbsp;


**Exploring inbetween charts with trajectory-guided sliders for cutout animation** \
*T Fukusato, A Maejima, T Igarashi, T Yotsukura*\
[1 Sep., 2023] [MTA 2023]

<a href="https://link.springer.com/article/10.1007/s11042-023-17354-x"><img src="./assets/paper.svg" alt="Icon" width="15" height="15"></a> &nbsp;


**Enhanced Deep Animation Video Interpolation** \
*Wang Shen, Cheng Ming, Wenbo Bao, Guangtao Zhai, Li Chen, Zhiyong Gao*\
[25 Jun., 2022] [arXiv, 2022]

<a href="https://arxiv.org/abs/2206.12657"><img src="./assets/paper.svg" alt="Icon" width="15" height="15"></a> &nbsp;


**Improving the Perceptual Quality of 2D Animation Interpolation** \
*Shuhong Chen, Matthias Zwicker*\
[24 Nov., 2021] [arXiv, 2021] 

<a href="https://arxiv.org/abs/2111.12792"><img src="./assets/paper.svg" alt="Icon" width="15" height="15"></a> &nbsp;


**Deep Animation Video Interpolation in the Wild** \
*Li Siyao, Shiyu Zhao, Weijiang Yu, Wenxiu Sun, Dimitris N. Metaxas, Chen Change Loy, Ziwei Liu*\
[6 Apr., 2021] [arXiv, 2021] 

<a href="https://arxiv.org/abs/2104.02495"><img src="./assets/paper.svg" alt="Icon" width="15" height="15"></a> &nbsp;
<a href="https://github.com/lisiyao21/AnimeInterp/"><img src="./assets/repo.svg" alt="Icon" width="15" height="15"></a> &nbsp;
<a href="https://github.com/lisiyao21/AnimeInterp/"><img src="./assets/dataset.svg" alt="Icon" width="15" height="15"></a> &nbsp;


**Deep Sketch-guided Cartoon Video Inbetweening** \
*Xiaoyu Li, Bo Zhang, Jing Liao, Pedro V. Sander*\
[10 Aug., 2020] [arXiv, 2020]

<a href="https://arxiv.org/abs/2008.04149"><img src="./assets/paper.svg" alt="Icon" width="15" height="15"></a> &nbsp;


**Optical Flow Based Line Drawing Frame Interpolation Using Distance Transform to Support Inbetweenings** \
*Rei Narita, Keigo Hirakawa, Kiyoharu Aizawa*\
[26 Aug., 2019] [IEEE, 2019]

<a href="https://ieeexplore.ieee.org/document/8803506"><img src="./assets/paper.svg" alt="Icon" width="15" height="15"></a> &nbsp;


**DiLight: Digital light table ‚Äì Inbetweening for 2D animations using guidelines** \
*Leonardo Carvalho, Ricardo Marroquim, Emilio Vital Brazil*\
[Jun., 2017] [Elsevier, 2017]

<a href="https://www.sciencedirect.com/science/article/abs/pii/S0097849317300390"><img src="./assets/paper.svg" alt="Icon" width="15" height="15"></a> &nbsp;



## Editing

**Re:Draw -- Context Aware Translation as a Controllable Method for Artistic Production** \
*Joao Liborio Cardoso, Francesco Banterle, Paolo Cignoni, Michael Wimmer*\
[Jan., 2024] [*TBA 2024]

<a href="https://arxiv.org/abs/2401.03499"><img src="./assets/paper.svg" alt="Icon" width="15" height="15"></a> &nbsp;


**Sprite-from-Sprite: Cartoon Animation Decomposition with Self-supervised Sprite Estimation** \
*Lvmin Zhang, Tien-Tsin Wong, Yuxin Liu*\
[Nov., 2022] [ACM 2022] 

<a href="https://dl.acm.org/doi/pdf/10.1145/3550454.3555439"><img src="./assets/paper.svg" alt="Icon" width="15" height="15"></a> &nbsp;
<a href="https://lllyasviel.github.io/GitPageToonDecompose/"><img src="./assets/repo.svg" alt="Icon" width="15" height="15"></a> &nbsp;


**Toonsynth: example-based synthesis of hand-colored cartoon animations** \
*M Dvoro≈æn√°k, W Li, VG Kim, D S√Ωkora*\
[Jul., 2018] [TOG 2018]

<a href="https://dl.acm.org/doi/abs/10.1145/3197517.3201326"><img src="./assets/paper.svg" alt="Icon" width="15" height="15"></a> &nbsp;


## Tracking & Matching

**Globally Optimal Toon Tracking** \
*Haichao Zhu, Xueting Liu, Tien-Tsin Wong, Pheng-Ann Heng* \
[11 Jul., 2016] [TOG, 2016]

<a href="https://dl.acm.org/doi/10.1145/2897824.2925872"><img src="./assets/paper.svg" alt="Icon" width="15" height="15"></a> &nbsp;


## Segmentation

**Stereoscopizing Cel Animations** \
*Xueting Liu, Xiangyu Mao, Xuan Yang, Linling Zhang, Tien-Tsin Wong* \
[11 Jul., 2016] [ACM, 2013]

<a href="https://dl.acm.org/doi/abs/10.1145/2508363.2508396"><img src="./assets/paper.svg" alt="Icon" width="15" height="15"></a> &nbsp;


## 3D Rotoscoping&Assistance

**Toon3D: Seeing Cartoons from a New Perspective** \
*Ethan Weber, Riley Peterlinz, Rohan Mathur, Frederik Warburg, Alexei A. Efros, Angjoo Kanazawa* \
[2024] [Arxiv, 2024]

<a href="https://www.arxiv.org/abs/2405.10320"><img src="./assets/paper.svg" alt="Icon" width="15" height="15"></a> &nbsp;
<a href="https://toon3d.studio/"><img src="./assets/webpage.svg" alt="Icon" width="15" height="15"></a> &nbsp;
<a href="https://www.youtube.com/watch?v=tJ7UKALsF-0&feature=youtu.be"><img src="./assets/demo.svg" alt="Icon" width="15" height="15"></a> &nbsp;
<a href="https://github.com/ethanweber/toon3d"><img src="./assets/repo.svg" alt="Icon" width="15" height="15"></a> &nbsp;
<a href="https://huggingface.co/datasets/ethanweber/toon3d-dataset">ü§ó</a> &nbsp;


## How to Contribute
We encourage animation enthusiasts, researchers, and scholars to contribute to this repository by adding relevant papers, articles, and resources. Your contributions will help build a valuable reference for anyone interested in the art and science of animation.

To contribute, simply fork this repository, make your additions or improvements, and submit a pull request. Please follow the contribution guidelines outlined in the repository's README file.

---

<div align="center">
    <em>May Animation Get Better With Us.</em>
</div>

<div align="center">
   <img src="https://github.com/zhenglinpan/Awesome-Animation-Research/blob/master/assets/Hatsune_Miku_@illufinch.png" width="40" >
</div>
<p align="center"><sub><i>icon by Twitter</i>¬©illufinch</sub></p>
