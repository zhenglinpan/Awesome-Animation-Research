![img1](https://github.com/zhenglinpan/Awesome-Animation-Research/blob/master/assets/teaser.gif)

[![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/amirhossein-kz/Awesome-Diffusion-Models-in-Medical-Imaging) 
[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](https://opensource.org/licenses/MIT)
[![Teaser](https://img.shields.io/badge/Teaser_by-å¯å›½-pink)](https://space.bilibili.com/177312952?spm_id_from=333.337.0.0)
<p align="left">
   <a href="https://github.com/zhenglinpan/Awesome-Animation-Research/blob/master/README.md">ENGLISH</a> | ç®€ä½“ä¸­æ–‡
</p>

# Awesome Animation Research ğŸ¥ğŸ“š

æœ¬repoæ”¶é›†äº†ä¸€æ‰¹å…³äº**ğŸï¸èµ›ç’çåŠ¨ç”»/ğŸï¸å¡é€šåŠ¨ç”»**çš„å„ç§ç ”ç©¶ã€æ•°æ®é›†åŠå…¶ç›¸å…³èµ„æºã€‚

ğŸ’â€â™€ï¸**åœ¨è¿™é‡Œä½ å¯ä»¥æ‰¾åˆ°:** æœ‰å¯èƒ½å¸®åŠ©åˆ°åŠ¨ç”»ä¸šç•Œçš„æŠ€æœ¯è®ºæ–‡ã€æ•°æ®é›†ã€repoç­‰ã€‚ä¾‹å¦‚ï¼šä¸­å‰²ç”Ÿæˆã€åŸç”»ä¸Šè‰²ç­‰ã€‚

ğŸ¤·â€â™€ï¸**è¿™ä¸ªrepoä¸åŒ…æ‹¬:** å¹¿ä¹‰çš„Animeç ”ç©¶ã€‚ä¾‹å¦‚ï¼šåŠ¨æ¼«é£æ ¼æ»¤é•œã€åŠ¨æ¼«å›¾åƒè¶…åˆ†ã€åŠ¨æ¼«äººç‰©ç”Ÿæˆç­‰ã€‚å¦‚æœä½ å¯¹å¹¿ä¹‰çš„Animeç ”ç©¶æ„Ÿå…´è¶£ï¼Œè¯·ç§»æ­¥[AwesomeAnimeResearch](https://github.com/SerialLain3170/AwesomeAnimeResearch).


****

ğŸ™‡â€â™€ï¸èµ›ç’çåŠ¨ç”»åˆ¶ä½œéå¸¸ä¸æ˜“ã€éœ€è¦å¤§é‡åŠ¨ç”»äººä¸€å¸§å¸§åœ°æ‰‹ç»˜ï¼Œè¿™ä¼šèŠ±è´¹å¤§é‡çš„æ—¶é—´å’Œç²¾åŠ›ã€‚è®¡ç®—æœºè§†è§‰æŠ€æœ¯ä¹Ÿè®¸æ­£åœ¨æ”¹å˜è¿™ä¸€ç°çŠ¶ï¼Œæœ‰ä¸å°‘çš„ç ”ç©¶è€…æ­£åœ¨å°è¯•åˆ©ç”¨è¿™ä¸€æŠ€æœ¯è¾…åŠ©åŠ¨ç”»åˆ¶ä½œä¸­çš„æŸäº›ç¯èŠ‚ï¼Œä¾‹å¦‚è‡ªåŠ¨ä¸­å‰²ã€è‡ªåŠ¨ä¸Šè‰²ç­‰ã€‚

ç›®å‰è¿™ä¸ªrepoåˆ—è¡¨ç›®å‰è¿˜å¾ˆçŸ­ï¼Œå› ä¸ºåŠ¨ç”»çš„è®¡ç®—æœºè§†è§‰ç ”ç©¶æ˜¯ä¸€ä¸ªç›¸å¯¹æ–°å…´ä¸”å°ä¼—çš„é¢†åŸŸï¼Œæˆ‘ä»¬æœŸå¾…æ›´å¤šçš„ç ”ç©¶è€…ï¼ˆä¹ŸåŒ…æ‹¬ä½ ï¼‰ä¸€èµ·ä¸ºè¿™ä¸ªé¢†åŸŸçš„å‘å±•åšå‡ºè´¡çŒ®ã€‚

æœ¬repoä¼šæŒç»­å…³æ³¨æœ€æ–°çš„ç ”ç©¶æˆæœï¼Œæ¬¢è¿å…³æ³¨! ğŸŒŸ

## æ–°æ–‡ç« 
<!-- [<span style="color:red">*new</span>]  -->

ğŸš©ã€æ•°æ®é›†ã€‘**Sakuga-42M Dataset: Scaling Up Cartoon Research** \
*Zhenglin Pan, Yu Zhu, Yuxuan Mu* \
[13 May., 2024] [arXiv, 2024] \
[[paper](https://arxiv.org/abs/2405.07425)] | [webpage] | [demo] | [[repo](https://github.com/zhenglinpan/SakugaDataset)] | [[dataset](https://github.com/zhenglinpan/SakugaDataset)]

ğŸš©ã€ä¸­å‰²ã€‘**Joint Stroke Tracing and Correspondence for 2D Animation** \
*Haoran Mo, Chengying Gao, Ruomei Wang*\
[9 Apr., 2024] [SIGGRAPH, 2024] \
[[paper](https://dl.acm.org/doi/10.1145/3649890)] | [[webpage](https://markmohr.github.io/JoSTC/)] | [demo] | [[repo](https://github.com/MarkMoHR/JoSTC)] | [dataset]

ğŸš©ã€ä¸Šè‰²ã€‘**Learning Inclusion Matching for Animation Paint Bucket Colorization** \
*Yuekun Dai, Shangchen Zhou, Qinyue Li, Chongyi Li, Chen Change Loy*\
[2024] [CVPR, 2024] \
[[paper](https://arxiv.org/abs/2403.18342)] | [[webpage](https://ykdai.github.io/projects/InclusionMatching)] | [[demo](https://www.youtube.com/watch?v=nNnPUItGvSo&ab_channel=YuekunDai)] | [[repo](https://github.com/ykdai/BasicPBC)] | [[dataset](https://github.com/ykdai/BasicPBC/tree/main/dataset)]

## æ•°æ®é›†

**Sakuga-42M Dataset: Scaling Up Cartoon Research** \
*Zhenglin Pan, Yu Zhu, Yuxuan Mu* \
[13 May., 2024] [arXiv, 2024] \
[[paper](https://arxiv.org/abs/2405.07425)] | [webpage] | [demo] | [[repo](https://github.com/zhenglinpan/SakugaDataset)] | [[dataset](https://github.com/zhenglinpan/SakugaDataset)]

**AnimeRun: 2D Animation Visual Correspondence from Open Source 3D Movies** \
*Li Siyao, Yuhang Li, Bo Li, Chao Dong, Ziwei Liu, Chen Change Loy* \
[10 Nov., 2022] [arXiv, 2022] \
[[paper](https://arxiv.org/abs/2211.05709)] | [[webpage](https://lisiyao21.github.io/projects/AnimeRun)] | [demo] | [[repo](https://github.com/lisiyao21/AnimeRun)] | [[dataset](https://lisiyao21.github.io/projects/AnimeRun)]


## ä¸Šè‰²

**Learning Inclusion Matching for Animation Paint Bucket Colorization** \
*Yuekun Dai, Shangchen Zhou, Qinyue Li, Chongyi Li, Chen Change Loy*\
[2024] [CVPR, 2024] \
[[paper](https://arxiv.org/abs/2403.18342)] | [[webpage](https://ykdai.github.io/projects/InclusionMatching)] | [[demo](https://www.youtube.com/watch?v=nNnPUItGvSo&ab_channel=YuekunDai)] | [[repo](https://github.com/ykdai/BasicPBC)] | [[dataset](https://github.com/ykdai/BasicPBC/tree/main/dataset)]

**Coloring anime line art videos with transformation region enhancement network** \
*Ning Wang, Muyao Niu, Zhi Dou, Zhihui Wang, Zhiyong Wang, Zhaoyan Ming, Bin Liu, Haojie Li*\
[Sep., 2023] [Elsevier, 2023] \
[[paper](https://www.sciencedirect.com/science/article/abs/pii/S0031320323002625)] | [webpage] | [demo] | [repo] | [dataset]

**SketchBetween: Video-to-Video Synthesis for Sprite Animation via Sketches** \
*Dagmar Lukka LoftsdÃ³ttir, Matthew Guzdial*\
[1 Sep., 2022] [ECCV, 2022] \
[[paper](https://arxiv.org/abs/2209.00185)] | [webpage] | [demo] | [[repo](https://github.com/ribombee/SketchBetween)] | [dataset]


**Animation Line Art Colorization Based on Optical Flow Method** \
*Yifeng Yu, Jiangbo Qian, Chong Wang, Yihong Dong, Baisong Liu*\
[27 Aug., 2022] [SSNR, 2022] \
[[paper](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4202289)] | [webpage] | [demo] | [repo] | [dataset]

**The Animation Transformer: Visual Correspondence via Segment Matching** \
*Evan Casey, VÃ­ctor PÃ©rez, Zhuoru Li, Harry Teitelman, Nick Boyajian, Tim Pulver, Mike Manh, William Grisaitis*\
[6 Sep., 2021] [arXiv, 2021] \
[[paper](https://arxiv.org/abs/2109.02614)] | [webpage] | [[demo](https://cadmium.app/)] | [repo] | [dataset]

**Artist-Guided Semiautomatic Animation Colorization** \
*Harrish Thasarathan, Mehran Ebrahimi* \
[22 Jun., 2020] [arXiv, 2020] \
[[paper](https://arxiv.org/abs/2006.13717)] | [webpage] | [demo] | [repo] | [dataset]

**Line Art Correlation Matching Feature Transfer Network for Automatic Animation Colorization** \
*Zhang Qian, Wang Bo, Wen Wei, Li Hai, Liu Jun Hui* \
[14 Apr., 2020] [arXiv, 2020] \
[[paper](https://arxiv.org/abs/2004.06718)] | [webpage] | [demo] | [repo] | [dataset]

**Deep Line Art Video Colorization with a Few References** \
*Min Shi, Jia-Qi Zhang, Shu-Yu Chen, Lin Gao, Yu-Kun Lai, Fang-Lue Zhang* \
[24 Mar., 2020] [arXiv, 2020] \
[[paper](https://arxiv.org/abs/2003.10685)] | [webpage] | [demo] | [repo] | [dataset]

**Automatic Temporally Coherent Video Colorization** \
*Harrish Thasarathan, Kamyar Nazeri, Mehran Ebrahimi* \
[[paper](https://arxiv.org/abs/1904.09527)] | [webpage] | [demo] | [[repo](https://github.com/Harry-Thasarathan/TCVC)] | [dataset]






## ä¸­å‰²/æ’å¸§

**Joint Stroke Tracing and Correspondence for 2D Animation** \
*Haoran Mo, Chengying Gao, Ruomei Wang*\
[9 Apr., 2024] [SIGGRAPH, 2024] \
[[paper](https://dl.acm.org/doi/10.1145/3649890)] | [[webpage](https://markmohr.github.io/JoSTC/)] | [demo] | [[repo](https://github.com/MarkMoHR/JoSTC)] | [dataset]

**Deep Geometrized Cartoon Line Inbetweening** \
*Li Siyao, Tianpei Gu, Weiye Xiao, Henghui Ding, Ziwei Liu, Chen Change Loy*\
[Nov., 2023] [ICCV 2023] \
[[paper](https://openaccess.thecvf.com/content/ICCV2023/html/Siyao_Deep_Geometrized_Cartoon_Line_Inbetweening_ICCV_2023_paper.html)] | [webpage] | [[demo](https://www.youtube.com/watch?v=iUF-LsqFKpI&ab_channel=SiyaoLi)] | [[repo](https://github.com/lisiyao21/AnimeInbet)] | [[dataset](https://drive.google.com/file/d/1SNRGajIECxNwRp6ZJ0IlY7AEl2mRm2DR/view)]


**Exploring inbetween charts with trajectory-guided sliders for cutout animation** \
*T Fukusato, A Maejima, T Igarashi, T Yotsukura*\
[1 Sep., 2023] [MTA 2023] \
[[paper](https://link.springer.com/article/10.1007/s11042-023-17354-x)]

**Enhanced Deep Animation Video Interpolation** \
*Wang Shen, Cheng Ming, Wenbo Bao, Guangtao Zhai, Li Chen, Zhiyong Gao*\
[25 Jun., 2022] [arXiv, 2022] \
[[paper](https://arxiv.org/abs/2206.12657)]

**Improving the Perceptual Quality of 2D Animation Interpolation** \
*Shuhong Chen, Matthias Zwicker*\
[24 Nov., 2021] [arXiv, 2021] \
[[paper](https://arxiv.org/abs/2111.12792)]

**Deep Animation Video Interpolation in the Wild** \
*Li Siyao, Shiyu Zhao, Weijiang Yu, Wenxiu Sun, Dimitris N. Metaxas, Chen Change Loy, Ziwei Liu*\
[6 Apr., 2021] [arXiv, 2021] \
[[paper](https://arxiv.org/abs/2104.02495)] | [[project](https://github.com/lisiyao21/AnimeInterp/)] | [[dataset](https://github.com/lisiyao21/AnimeInterp/)]

**Deep Sketch-guided Cartoon Video Inbetweening** \
*Xiaoyu Li, Bo Zhang, Jing Liao, Pedro V. Sander*\
[10 Aug., 2020] [arXiv, 2020] \
[[paper](https://arxiv.org/abs/2008.04149)] | [webpage] | [demo] | [repo] | [dataset]

**Optical Flow Based Line Drawing Frame Interpolation Using Distance Transform to Support Inbetweenings** \
*Rei Narita, Keigo Hirakawa, Kiyoharu Aizawa*\
[26 Aug., 2019] [IEEE, 2019] \
[[paper](https://ieeexplore.ieee.org/document/8803506)] | [webpage] | [demo] | [repo] | [dataset]

**DiLight: Digital light table â€“ Inbetweening for 2D animations using guidelines** \
*Leonardo Carvalho, Ricardo Marroquim, Emilio Vital Brazil*\
[Jun., 2017] [Elsevier, 2017] \
[[paper](https://www.sciencedirect.com/science/article/abs/pii/S0097849317300390)] | [webpage] | [demo] | [repo] | [dataset]




## ç¼–è¾‘

**Re:Draw -- Context Aware Translation as a Controllable Method for Artistic Production** \
*Joao Liborio Cardoso, Francesco Banterle, Paolo Cignoni, Michael Wimmer*\
[Jan., 2024] [*TBA 2024] \
[[paper](https://arxiv.org/abs/2401.03499)] | [webpage] | [demo] | [repo] | [dataset]

**Sprite-from-Sprite: Cartoon Animation Decomposition with Self-supervised Sprite Estimation** \
*Lvmin Zhang, Tien-Tsin Wong, Yuxin Liu*\
[Nov., 2022] [ACM 2022] \
[[paper](https://dl.acm.org/doi/pdf/10.1145/3550454.3555439)] | [webpage] | [demo] | [[repo](https://lllyasviel.github.io/GitPageToonDecompose/)] | [dataset]

**Toonsynth: example-based synthesis of hand-colored cartoon animations** \
*M DvoroÅ¾nÃ¡k, W Li, VG Kim, D SÃ½kora*\
[Jul., 2018] [TOG 2018] \
[[paper](https://dl.acm.org/doi/abs/10.1145/3197517.3201326)] | [webpage] | [demo] | [repo] | [dataset]

## è·Ÿè¸ª/åŒ¹é…

**Globally Optimal Toon Tracking** \
*Haichao Zhu, Xueting Liu, Tien-Tsin Wong, Pheng-Ann Heng* \
[11 Jul., 2016] [TOG, 2016] \
[[paper](https://dl.acm.org/doi/10.1145/2897824.2925872)] | [webpage] | [demo] | [repo] | [dataset]

## åˆ†å‰²

**Stereoscopizing Cel Animations** \
*Xueting Liu, Xiangyu Mao, Xuan Yang, Linling Zhang, Tien-Tsin Wong* \
[11 Jul., 2016] [ACM, 2013] \
[[paper](https://dl.acm.org/doi/abs/10.1145/2508363.2508396)] | [webpage] | [demo] | [repo] | [dataset]

## å¦‚ä½•Contribute 
æˆ‘ä»¬é¼“åŠ±åŠ¨ç”»çˆ±å¥½è€…ã€ç ”ç©¶è€…é€šè¿‡æ·»åŠ ç›¸å…³è®ºæ–‡ã€æ–‡ç« å’Œå„ç±»èµ„æºçš„å½¢å¼ä¸ºæœ¬èµ„æ–™åº“åšå‡ºè´¡çŒ®ã€‚æ‚¨çš„è´¡çŒ®å°†æœ‰åŠ©äºä¸ºä»»ä½•å¯¹åŠ¨ç”»ç ”ç©¶æ„Ÿå…´è¶£çš„äººæä¾›æœ‰ä»·å€¼çš„å‚è€ƒã€‚

åªéœ€ fork æœ¬èµ„æºåº“ï¼Œè¿›è¡Œæ·»åŠ æˆ–æ”¹è¿›ï¼Œå¹¶pull requestå³å¯ã€‚

---

<div align="center">
    <em>å¸Œæœ›åŠ¨ç”»å› æˆ‘ä»¬è€Œæ›´å¥½.</em>
</div>

<div align="center">
   <img src="https://github.com/zhenglinpan/Awesome-Animation-Research/blob/master/assets/Hatsune_Miku_@illufinch.png" width="40" >
</div>
<p align="center"><sub><i>å›¾æ ‡ by Twitter</i>Â©illufinch</sub></p>
